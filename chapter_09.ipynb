{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9章 潜在顧客を把握するための画像認識10本ノック"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下準備として，データのあるディレクトリに移動しておく\n",
    "import os\n",
    "\n",
    "DATA_ROOT_DIR = \"./sample/9章/\"\n",
    "os.chdir(DATA_ROOT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ノック81 画像データを読み込んでみよう\n",
    "まずはデータを読み込む．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "画像幅: 1920\n画像高さ: 1440\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from IPython.display import Image, display\n",
    "img = cv2.imread(\"img/img01.jpg\")\n",
    "height, width = img.shape[:2]\n",
    "print(\"画像幅: \" + str(width))\n",
    "print(\"画像高さ: \" + str(height))\n",
    "\n",
    "# display(Image(\"img/img01.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ノック８２：映像データを読み込んでみよう\n",
    "やはりデータを読み込む．"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "画像幅: 1920.0\n画像高さ: 1440.0\n総フレーム数: 401.0\nFPS: 30.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from IPython.display import Video, display, YouTubeVideo, HTML\n",
    "\n",
    "# 情報取得\n",
    "cap = cv2.VideoCapture(\"mov/mov01.avi\")\n",
    "width = cap.get(cv2.CAP_PROP_FRAME_WIDTH)\n",
    "height = cap.get(cv2.CAP_PROP_FRAME_HEIGHT)\n",
    "count = cap.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "print(\"画像幅: \" + str(width))\n",
    "print(\"画像高さ: \" + str(height))\n",
    "print(\"総フレーム数: \" + str(count))\n",
    "print(\"FPS: \" + str(fps))\n",
    "\n",
    "# 出力\n",
    "# while(cap.isOpened()):\n",
    "#     ret, frame = cap.read()\n",
    "#     if ret:\n",
    "#         cv2.imshow(\"frame\", frame)\n",
    "#     if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "#         break\n",
    "cap.release()\n",
    "# cv2.destroyAllWindows()\n",
    "\n",
    "# display(Video(\"mov/mov01.avi\", embed=True))"
   ]
  },
  {
   "source": [
    "## ノック83 映像を画像に分割し、保存してみよう\n",
    "動画から画像を切り抜く\n",
    "\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "cap = cv2.VideoCapture(\"mov/mov01.avi\")\n",
    "num = 0\n",
    "while(cap.isOpened()):\n",
    "    ret, frame = cap.read()\n",
    "    if ret:\n",
    "        # cv2.imshow(\"frame\", frame)\n",
    "        filepath = \"snapshot/snapshot_\" + str(num) + \".jpg\"\n",
    "        if not os.path.exists(filepath):\n",
    "            cv2.imwrite(filepath,frame)\n",
    "        # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        #     break\n",
    "    else:\n",
    "        break\n",
    "    num = num + 1\n",
    "cap.release()\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "source": [
    "## ノック84 画像内のどこに人がいるのかを検出してみよう\n",
    "HOG特徴量を利用して人の検出を行う．"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# 準備\n",
    "hog = cv2.HOGDescriptor()\n",
    "hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "hogParams = {'winStride': (8, 8), 'padding': (32, 32), 'scale': 1.05, 'hitThreshold':0, 'finalThreshold':5}\n",
    "\n",
    "# 検出\n",
    "img = cv2.imread(\"img/img01.jpg\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "human, r = hog.detectMultiScale(gray, **hogParams)\n",
    "if (len(human)>0):\n",
    "    for (x, y, w, h) in human:\n",
    "        cv2.rectangle(img, (x, y), (x + w, y + h), (255,255,255), 3)\n",
    "# cv2.imshow(\"img\",img)\n",
    "_ = cv2.imwrite(\"temp.jpg\",img)\n",
    "# display(Image(\"temp.jpg\"))"
   ]
  },
  {
   "source": [
    "## ノック85 画像内の人の顔を検出してみよう\n",
    "Haar-like特徴量を利用して顔検出を行う．"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from IPython.display import Image, display\n",
    "\n",
    "# 準備\n",
    "cascade_file = \"haarcascade_frontalface_alt.xml\"\n",
    "cascade = cv2.CascadeClassifier(cascade_file)\n",
    "\n",
    "# 検出\n",
    "img = cv2.imread(\"img/img02.jpg\")\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "face_list = cascade.detectMultiScale(gray, minSize=(50, 50))\n",
    "\n",
    "for (x, y, w, h) in face_list:\n",
    "    color = (0, 0, 225)\n",
    "    pen_w = 3\n",
    "    cv2.rectangle(img, (x, y), (x+w, y+h), color, thickness = pen_w)\n",
    "\n",
    "# cv2.imshow(\"img\",img)\n",
    "_ = cv2.imwrite(\"temp.jpg\",img)\n",
    "# cv2.waitKey(0)\n",
    "# display(Image(\"temp.jpg\"))"
   ]
  },
  {
   "source": [
    "## ノック86 画像内の人がどこに顔を向けているのかを検出してみよう\n",
    "dlibで顔のランドマークを検出し，顔の向きを推定する．"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "顔方位:0.06456096931747406 (角度:3.6990710631648662度)\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import math\n",
    "\n",
    "# 準備 #\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# 検出 #\n",
    "img = cv2.imread(\"img/img02.jpg\")\n",
    "dets = detector(img, 1)\n",
    "\n",
    "for k, d in enumerate(dets):\n",
    "    shape = predictor(img, d)\n",
    "\n",
    "    # 顔領域の表示\n",
    "    color_f = (0, 0, 225)\n",
    "    color_l_out = (255, 0, 0)\n",
    "    color_l_in = (0, 255, 0)\n",
    "    line_w = 3\n",
    "    circle_r = 3\n",
    "    fontType = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    fontSize = 1\n",
    "    cv2.rectangle(img, (d.left(), d.top()), (d.right(), d.bottom()), color_f, line_w)\n",
    "    cv2.putText(img, str(k), (d.left(), d.top()), fontType, fontSize, color_f, line_w)\n",
    "\n",
    "    # 重心を導出する箱を用意\n",
    "    num_of_points_out = 17\n",
    "    num_of_points_in = shape.num_parts - num_of_points_out\n",
    "    gx_out = 0\n",
    "    gy_out = 0\n",
    "    gx_in = 0\n",
    "    gy_in = 0\n",
    "    for shape_point_count in range(shape.num_parts):\n",
    "        shape_point = shape.part(shape_point_count)\n",
    "        #print(\"顔器官No.{} 座標位置: ({},{})\".format(shape_point_count, shape_point.x, shape_point.y))\n",
    "        #器官ごとに描画\n",
    "        if shape_point_count<num_of_points_out:\n",
    "            cv2.circle(img,(shape_point.x, shape_point.y),circle_r,color_l_out, line_w)\n",
    "            gx_out = gx_out + shape_point.x/num_of_points_out\n",
    "            gy_out = gy_out + shape_point.y/num_of_points_out\n",
    "        else:\n",
    "            cv2.circle(img,(shape_point.x, shape_point.y),circle_r,color_l_in, line_w)\n",
    "            gx_in = gx_in + shape_point.x/num_of_points_in\n",
    "            gy_in = gy_in + shape_point.y/num_of_points_in\n",
    "\n",
    "    # 重心位置を描画\n",
    "    cv2.circle(img,(int(gx_out), int(gy_out)),circle_r,(0,0,255), line_w)\n",
    "    cv2.circle(img,(int(gx_in), int(gy_in)),circle_r,(0,0,0), line_w)\n",
    "\n",
    "    # 顔の方位を計算\n",
    "    theta = math.asin(2*(gx_in-gx_out)/(d.right()-d.left()))\n",
    "    radian = theta*180/math.pi\n",
    "    print(\"顔方位:{} (角度:{}度)\".format(theta,radian))\n",
    "\n",
    "    # 顔方位を表示\n",
    "    if radian<0:\n",
    "        textPrefix = \"   left \"\n",
    "    else:\n",
    "        textPrefix = \"   right \"\n",
    "    textShow = textPrefix + str(round(abs(radian),1)) + \" deg.\"\n",
    "    cv2.putText(img, textShow, (d.left(), d.top()), fontType, fontSize, color_f, line_w)\n",
    "\n",
    "\n",
    "# cv2.imshow(\"img\",img)\n",
    "_ = cv2.imwrite(\"temp.jpg\",img)\n",
    "# display(Image(\"temp.jpg\"))"
   ]
  },
  {
   "source": [
    "## ノック87 検出した情報を統合し、タイムラプスを作ってみよう\n",
    "検出結果のタイムラプスを作成する．"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video, display, YouTubeVideo, HTML\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "movie_name = \"timelapse.avi\"\n",
    "if not os.path.exists(movie_name):\n",
    "    print(\"タイムラプス生成開始\")\n",
    "\n",
    "    # 映像取得\n",
    "    cap = cv2.VideoCapture(\"mov/mov01.avi\")\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # hog\n",
    "    hog = cv2.HOGDescriptor()\n",
    "    hog.setSVMDetector(cv2.HOGDescriptor_getDefaultPeopleDetector())\n",
    "    hogParams = {'winStride': (8, 8), 'padding': (32, 32), 'scale': 1.05, 'hitThreshold':0, 'finalThreshold':5}\n",
    "\n",
    "    # タイムラプス作成\n",
    "    fourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D')\n",
    "    video = cv2.VideoWriter(movie_name,fourcc, 30, (width,height))\n",
    "\n",
    "    num = 0\n",
    "    while(cap.isOpened()):\n",
    "        ret, frame = cap.read()\n",
    "        if ret:\n",
    "            if (num%10==0):\n",
    "                gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "                human, r = hog.detectMultiScale(gray, **hogParams)\n",
    "                if (len(human)>0):\n",
    "                    for (x, y, w, h) in human:\n",
    "                        cv2.rectangle(frame, (x, y), (x + w, y + h), (255,255,255), 3)\n",
    "\n",
    "                video.write(frame)\n",
    "        else:\n",
    "            break\n",
    "        num = num + 1\n",
    "    video.release()\n",
    "    cap.release()\n",
    "    # cv2.destroyAllWindows()\n",
    "    print(\"タイムラプス生成終了\")\n",
    "# display(Video(\"timelapse.avi\", embed=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}